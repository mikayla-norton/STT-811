---
title: "ICAs"
author: "Mikayla Norton"
date: "2023-01-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r fileUpload}
library(sqldf)
library(tidyverse)
order_details <- read.csv("orders/order_details.csv")
orders <- read.csv("orders/orders.csv")
territories <- read.csv("orders/territories.csv")
regions <- read.csv("orders/regions.csv")
employee_territories <- read.csv("orders/employee_territories.csv")
employees <- read.csv("orders/employees.csv")
customers <- read.csv("orders/customers.csv")
shippers <- read.csv("orders/shippers.csv")
suppliers <- read.csv("orders/suppliers.csv")
products <- read.csv("orders/products.csv")
categories <- read.csv("orders/categories.csv")

customer_history <- read.csv("orders/Customer_History.csv")
order_history <- read.csv("orders/Order_History.csv")
oh_un <- read.csv("orders/ProductOrderUndated.csv")
shipper_history <- read.csv("orders/Shipper_history.csv")
shipping_history <- read.csv("orders/Shipping_history.csv")
complaints <- read.csv("orders/Complaint_History.csv")
```
# completed
```{r ica1}

#1.
low_ship <- sqldf("SELECT * from orders
                  WHERE ShipVia = 1")
#2.
order_summary <- sqldf("SELECT productID, avg(unitPrice)
                       FROM order_details
                       GROUP BY productID")
order_summary
```
```{r ica2}
#1
regionsTerritories <- sqldf("SELECT territories.*, regionDescription
                            FROM regions
                            INNER JOIN territories
                            WHERE regions.regionID = territories.regionID")

#2

customerOrders <- sqldf("SELECT orderID, companyName, country
                        FROM customers
                        INNER JOIN orders
                        ON customers.customerID = orders.customerID")

#3
customerDetails <- sqldf("SELECT order_details.*, companyName
                         FROM order_details
                         INNER JOIN customers
                         INNER JOIN orders
                         ON order_details.orderID = orders.orderID
                         AND orders.customerID = customers.customerID")

#4
supplierShipper <- sqldf("SELECT *
                         FROM shippers
                         INNER JOIN suppliers")

#5
employeesToCustomers <- sqldf("SELECT companyName AS customerName, customers.country, firstName, lastName
                              FROM customers
                              INNER JOIN employees
                              INNER JOIN orders
                              ON orders.employeeID = employees.employeeID
                              AND orders.customerID = customers.customerID")

employeesToCustomers
```
```{r ica3}
#1
years <- data.frame(year = seq(2005,2022))


customer_history2 <- sqldf("SELECT *
               FROM customer_history
               INNER JOIN years
               WHERE year <= Last_Year 
               AND year >= First_Year")


#2
order_history2 <- sqldf("SELECT CustomerID, Product_ID, Year, Month, sum(Quantity) as Quantity
                        FROM order_history
                        GROUP BY customerID")

order_wide <- pivot_wider(order_history2, names_from = Year, values_from = Quantity)
order_long <- pivot_longer(order_wide, cols = starts_with('20'), names_to = 'Year', values_to = 'Quantity',)
order_long <- sqldf("SELECT *
                    FROM order_long
                    ORDER BY Year, Month, CustomerID, Product_ID")
```
```{r 1.19notes}
# add extra year column to cust_hist
years <- data.frame(year = seq(2005,2022))


customer_history2 <- sqldf("SELECT *
               FROM customer_history
               INNER JOIN years
               WHERE year <= Last_Year 
               AND year >= First_Year")

# summary of complaints
complaintsSummary <- sqldf("SELECT *,COUNT(*) as ComplaintCount
                           FROM complaints
                           GROUP BY customerID, ComplaintYear
                           ORDER BY ComplaintYear, customerID")

#Individual Year Approach
predictions <- sqldf("SELECT *, ComplaintYear+1 as PredictiveYear
                     FROM complaintsSummary")

ComplaintsCount <- sqldf("SELECT *, sum(ComplaintCount) as ComplaintHist
                   FROM predictions
                   GROUP BY PredictiveYear>ComplaintYear")

#Aggregate Approach
aggregateComplaints <- sqldf("SELECT t1.ComplaintYear, t2.ComplaintYear as PredictiveYear, COUNT(*)
                             FROM complaints t1
                             INNER JOIN complaints t2
                             WHERE t2.ComplaintYear > t1.ComplaintYear
                             AND t2.ComplaintYear <= t1.ComplaintYear+1
                             GROUP BY t1.customerID, t1.ComplaintYear
                             ORDER BY t1.ComplaintYear")
```
```{r ica4}

complaintsSummary <- sqldf("SELECT *,COUNT(*) as ComplaintCount
                           FROM complaints
                           GROUP BY customerID, ComplaintYear
                           ORDER BY ComplaintYear, customerID")

predictions <- sqldf("SELECT *, ComplaintYear+1 as PredictiveYear
                     FROM complaintsSummary")


ComCount <- sqldf("SELECT CustomerID, PredictiveYear,
                      SUM (SUM (ComplaintCount)) OVER (ORDER BY PredictiveYear) AS ComplaintHist
                    FROM predictions
                    GROUP BY CustomerID, PredictiveYear
                    ORDER BY CustomerID, PredictiveYear")
View(ComCount)
```
```{r ica5}
salesRaw <- read.csv("sales_data.csv")


sales <- sqldf("SELECT X, 
               CASE WHEN units = 'kg' ")

```
```{r ica6}
heart <- read.csv("Heart.csv")
head(heart)
```
```{r ica7}
heart <- read.csv("Heart.csv")
library(caret)
library(ggplot2)
heart$y <- ifelse(heart$AHD=="Yes",1,0)

heart_mod <- glm(data = heart, y ~ MaxHR + RestBP + as.factor(ChestPain), family = binomial)
summary(heart_mod)

b0 = 2.915699
b1 = -0.032991
b2 = 0.021750
b3 = -2.010243
x1 = 170
x2 = 145
y1 <- 1/(1+exp(-(b0+b1*x1+b2*x2+b3)))
confusionMatrix(data = as.factor(as.integer(2*heart_mod$fitted.values)), reference = as.factor(heart$y))

odds <- p/(1-p)
odds
```
```{r ica8}
split_pct <- 0.75
n <- length(OJ$Purchase1)*split_pct # train size
row_samp <- sample(1:length(OJ$Purchase1), n, replace = FALSE)
train <- OJ[row_samp,]
test <- OJ[-row_samp,]
OJ_train_mod <- glm(data = train, Purchase1 ~ PriceDiff + LoyalCH, family = binomial)
test_pred <- predict(OJ_train_mod,test, type = "response")
train_cm <- confusionMatrix(as.factor(as.integer(2*OJ_train_mod$fitted.values)), reference = as.factor(train$Purchase1))
test_cm <- confusionMatrix(as.factor(as.integer(2*test_pred)), reference = as.factor(test$Purchase1))
train_cm$table
test_cm$table
```
```{r ica11}
library(class)
library(caret)
oj <- ISLR2::OJ
split_pct <- 0.75
n <- length(oj$Purchase)*split_pct # train size
row_samp <- sample(1:length(oj$Purchase), n, replace = FALSE)
train <- oj[row_samp,c(1,10,13)]
test <- oj[-row_samp,c(1,10,13)]

###
train.Y = train$Purchase
test.Y = test$Purchase
train_scale = scale(train[,c(2,3)])
test_scale = scale(test[,c(2,3)])
knn_mod<-knn(train = train_scale, test = test_scale, cl = train.Y, k=5)

cm<-confusionMatrix(knn_mod, reference = as.factor(test.Y))
cm$table
```

```{r ica13}
library(tree)
library(caret)
heart <- read.csv("data/Heart.csv")

split_pct <- 0.75
n <- length(heart$AHD)*split_pct # train size
row_samp <- sample(1:length(heart$AHD), n, replace = FALSE)
train <- heart[row_samp,]
test <- heart[-row_samp,]

Heart_tree <- tree(as.factor(AHD) ~ RestBP + Age + as.factor(Thal) + as.factor(Sex), data = train)


plot(Heart_tree)
text(Heart_tree)
tree_pred <- predict(Heart_tree, test, type = 'class')

confusionMatrix(tree_pred, as.factor(test$AHD))
```
```{r ica14}
library(ISLR2)
library(tree)
library(randomForest)
library(caret)

weekly <- ISLR2::Weekly

# Ups<-weekly[weekly["Direction"] == 'Up',]
# Downs<-weekly[weekly["Direction"] == 'Down',]
# subset_pct <- 0.8
# n <- length(Ups$Direction)*subset_pct # train size
# row_samp <- sample(1:length(Ups$Direction), n, replace = FALSE)
# subset_ups <- Ups[row_samp,]
# 
# Subsetted_weekly <- rbind(Downs, subset_ups)



split_pct <- 0.75
n <- length(weekly$Direction)*split_pct # train size
row_samp <- sample(1:length(weekly$Direction), n, replace = FALSE)
train <- weekly[row_samp,]
test <- weekly[-row_samp,]




weekly_tree <- tree(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = train)
tree_pred <- predict(weekly_tree, test, type = 'class')
confusionMatrix(tree_pred, test$Direction)


predicts <- matrix(nrow = length(test$Direction), ncol = 0)
for(i in 1:3){
  rows <- sample(1:length(train$Direction), length(train$Direction), replace = TRUE)
  samp <- weekly[rows,]
  weekly_trees <- tree(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = samp)
  predicts = cbind(predicts, predict(weekly_trees, test)[,1])
}
ens <- rowMeans(predicts)

confusionMatrix(as.factor(ifelse(ens < 0.5, 'Up', 'Down')), reference = test$Direction)


# weekly_rf <- randomForest(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = train, mtry = 3, importance = TRUE, ntree = 500, maxnodes = 4)
# rf_predict <- predict(weekly_rf, test)
# confusionMatrix(rf_predict, test$Direction)
# barplot(weekly_rf$importance[,3])
```



```{r ica15}
library(ISLR2)
library(xgboost)
weekly <- ISLR2::Weekly
#View(weekly)
weekly$Direction <- as.integer(ifelse(weekly$Direction == "Down", 1, 0))
split_pct <- 0.75
n <- length(weekly$Direction)*split_pct
row_samp <- sample(1:length(weekly$Direction), n, replace = FALSE)
train <- weekly[row_samp,]
test <- weekly[-row_samp,]
```

```{r ica15p2}
weekly_xgb <- xgboost(data = data.matrix(train[,c(2:7)]), nrounds = 50, max_depth = 3, eta = 0.1, label = train$Direction, objective = "binary:logistic")
```

```{r ica15p3}
pred <- predict(weekly_xgb, data.matrix(test[,c(2:7)]))
confusionMatrix(as.factor(as.integer(2*pred)), as.factor(test$Direction))
```
```{r ica17}
library(e1071)
library(caret)
heart <- read.csv("data/Heart.csv")
heart$AHD <- as.numeric(as.factor(heart$AHD))
split_pct <- 0.7
n <- length(heart$AHD)*split_pct # train size
row_samp <- sample(1:length(heart$AHD), n, replace = FALSE)
train <- heart[row_samp,]
test <- heart[-row_samp,]

svm_mod <- svm(AHD ~ RestBP + MaxHR, data = train, type = 'C-classification', kernel = 'linear', cost = 4, gamma = 1)

```

```{r ica18}
heart <- read.csv("data/Heart.csv")
heart <- na.omit(heart)
heart_sc <- scale(heart[,c(2,3,5:13)])
heart_km <- kmeans(heart_sc, centers = 8, nstart = 25)
heart_km$size
```


```{r ica19}
airline <- read.csv("data/passenger.csv.csv")
airline_ts <- ts(airline$time, frequency = 12, c(1949,1))

acf(airline_ts)
pacf(airline_ts)

oilgas <- read.csv("data/oil-gas.csv")
oil_ts_week <- ts(oilgas$Date, frequency = 52, c(2013,1))

oil_ts_yr <- ts(oilgas$Date, frequency = 12, c(2013,4))
```
```{r ica20}
beer <- read.csv("data/beer.csv")
beer_ts <- ts(beer$Monthly.beer.production, frequency = 12, start = c(1956, 1))

tsn_beer <- decompose(beer_ts, type = "multiplicative")
plot(tsn_beer)

###
library(forecast)
beer_naive <- naive(beer_ts)
plot(beer_naive)
mean(abs(beer_naive$residuals)/beer_ts)

# seasonal naive model
beer_snaive <- snaive(beer_ts, h = 12)
plot(beer_snaive)
mean(abs(beer_snaive$residuals)/beer_ts)



# simple exponential smoothing
beer_ses <- ses(beer_ts)
plot(beer_ses)
mean(abs(beer_ses$residuals)/beer_ts)


# holt model
beer_holt <- holt(beer_ts)
plot(beer_holt)
mean(abs(beer_holt$residuals)/beer_ts)


# holt-winters
beer_hw <- hw(beer_ts, h = 12)
mean(abs(beer_hw$residuals)/beer_ts)
plot(beer_hw)
plot(beer_ts)

```




#

```{r ica21}
library(forecast)

beer <- read.csv("data/beer.csv")
beer_ts <- ts(beer$Monthly.beer.production, frequency = 12, start = c(1956, 1))
plot(beer_ts)


acf(beer_ts)
pacf(beer_ts)


beer_dec <- decompose(beer_ts, type = 'multiplicative')
rand_beer <- ts(beer_dec$random[13:138], start = c(1956,1), frequency = 12)
aa_beer<-auto.arima(rand_beer)


mean(abs(aa_beer$residuals)/beer_ts)
```